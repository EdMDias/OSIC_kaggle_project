{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pydicom \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "#import keras\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Concatenate, Dense, Input, concatenate, BatchNormalization\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv3D, MaxPool3D, AveragePooling3D\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_best_slices(path):\n",
    "    ind = []\n",
    "    for file in os.listdir(path):\n",
    "        if file.split('.')[1] == 'npy':\n",
    "            ind.append(file.split('.')[0])\n",
    "        \n",
    "    df = pd.DataFrame(index = ind, columns= ['CT'])\n",
    "    for ind in df.index:\n",
    "        df.loc[ind].CT = np.load(path + ind + '.npy')\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = load_best_slices('lung_chunks/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss_function():\n",
    "\n",
    "    def loss_function(y_true, y_pred):\n",
    "\n",
    "        diff = abs(y_pred-y_true)\n",
    "        \n",
    "        return tf.reduce_mean(diff**4, axis=-1)**.25\n",
    "    \n",
    "    return loss_function\n",
    "\n",
    "def custom_metric_function():\n",
    "\n",
    "    def metric_function(y_true, y_pred):\n",
    "        \n",
    "        diff = abs(y_pred-y_true)\n",
    "        diff = tf.where(diff > 7.5, 7.5, diff)\n",
    "        diff = tf.where(diff < 0.5, 0.5, diff)\n",
    "        \n",
    "        return tf.reduce_mean(diff, axis=-1)\n",
    "    \n",
    "    return metric_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()\n",
    "df = df.rename(columns = {'index' : 'Patient'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.read_csv('features.csv')\n",
    "df = df.merge(features, on= 'Patient')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_data_all = pd.read_csv('patient_slope_intercept.csv', index_col=0)\n",
    "result = pd.DataFrame(index = df.Patient, columns = ['slope'])\n",
    "    \n",
    "for ind in result.index:\n",
    "    result.loc[ind].slope = linear_data_all.loc[ind].slope\n",
    "\n",
    "df = df.merge(result, on='Patient')\n",
    "df.iloc[:,2:] = df.iloc[:,2:] #.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df.values[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_features = MinMaxScaler()\n",
    "dataset[:,1:-1] = scaler_features.fit_transform(dataset[:,1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dataset[:,-1].copy().astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.4362868e+01, -1.3071039e+00, -7.3706927e+00, -8.5644579e+00,\n",
       "        1.3588895e+00, -4.2086034e+00,  1.4789916e+00, -3.7748799e-02,\n",
       "       -1.4086339e+00, -2.8182575e+01,  4.1800714e+00,  9.0522079e+00,\n",
       "        2.5571496e+00, -1.4488882e+00, -1.5641643e+00, -2.5840535e+00,\n",
       "       -3.0797420e+00, -1.8381779e+01, -1.6121746e+01, -8.9318962e+00,\n",
       "       -6.5893281e-01, -3.3735905e+00, -5.9531841e+00,  6.9377160e-01,\n",
       "       -2.3159204e+00, -8.9127941e+00, -1.9162534e-01, -5.1311207e+00,\n",
       "       -1.7042803e+01, -5.6847239e-01, -5.7333183e+00, -2.3287500e+01,\n",
       "        7.3312583e+00, -5.8735952e+00, -8.1894598e+00, -1.0402375e+00,\n",
       "       -2.0166771e+00, -9.3818474e+00, -1.0939918e+01, -1.0805558e+00,\n",
       "       -3.1304660e+00, -9.5680344e-01, -3.4443293e+00, -9.2148857e+00,\n",
       "       -8.0079710e-01, -4.3741259e-01, -4.6877728e+00, -1.5106828e+00,\n",
       "       -3.4103143e+00, -2.3561151e+00, -4.7401042e+00, -8.5430784e+00,\n",
       "       -1.5101547e+00,  1.8965999e+00, -9.2992169e-01, -7.5789595e+00,\n",
       "       -2.9678593e+00, -1.4017116e+00,  3.5006383e-01, -5.5858965e+00,\n",
       "       -2.2809870e+00, -4.2896276e+00, -5.9863248e+00,  6.5638214e-01,\n",
       "       -8.5157490e+00, -1.0778019e+01, -1.1424745e+00, -5.9479108e+00,\n",
       "        1.6659638e-02, -4.0704088e+00, -1.1790502e+00, -9.1075821e+00,\n",
       "       -8.8379431e-01, -1.8502907e+00, -3.6462543e+00, -1.3420945e+01,\n",
       "       -4.5489249e+00, -7.6172199e+00, -2.2209294e+00, -5.5308867e+00,\n",
       "       -8.1157036e+00, -3.8140481e+00, -3.8737373e+00, -3.9566951e+00,\n",
       "       -1.1627400e+01, -9.3799553e+00, -4.9803667e+00, -1.6464817e+01,\n",
       "       -4.2635021e+00, -5.4943686e+00, -9.0814133e+00,  1.4750060e+00,\n",
       "       -1.2683187e+01, -5.7299609e+00, -6.7118049e-02, -4.1850462e+00,\n",
       "       -4.8080912e+00, -2.5491517e+00,  1.1225947e+01, -1.0813172e+00,\n",
       "       -1.5030663e+01, -4.4062743e+00, -4.4715085e+00, -2.0348134e+00,\n",
       "       -7.4604788e+00,  4.0556315e-01, -8.2244892e+00, -1.5329545e+01,\n",
       "       -4.2404294e+00, -8.1392450e+00,  7.1656978e-01,  1.9887955e+00,\n",
       "        4.6776166e+00, -5.1661301e+00, -4.2121315e+00,  1.6565740e+00,\n",
       "       -2.4291012e+01,  2.5611193e+00, -3.5064619e+00, -8.2876348e+00,\n",
       "       -2.3114529e+01, -4.5130491e+00, -9.6731586e+00, -1.1658906e+01,\n",
       "       -2.4566929e-01, -3.3056822e-01,  4.5155005e+00, -1.4033412e+01,\n",
       "       -2.3353024e+00, -6.5318942e+00, -8.9405336e+00, -5.1659112e+00,\n",
       "       -5.7674104e-01, -2.6516074e-01,  6.8111193e-01,  5.9813386e-01,\n",
       "        2.6533803e-01, -4.4978418e+00, -2.2168515e+00, -6.3146906e+00,\n",
       "       -1.1616005e+00, -3.9444821e+00, -6.4033785e+00, -7.1823525e+00,\n",
       "       -2.0453682e+01, -6.7418227e+00,  3.9701631e+00,  7.1971378e+00,\n",
       "       -3.3800275e+00, -2.8084153e-01, -3.6651213e+00, -1.0818152e+01,\n",
       "       -3.1671259e+00, -9.7565527e+00,  3.2038146e-01, -7.0297799e+00,\n",
       "       -1.2898512e-01,  1.4682611e+01, -4.1856961e+00, -6.0414400e+00,\n",
       "       -1.1805201e+01, -2.8371396e+00,  8.2416420e+00, -3.4435780e+00,\n",
       "       -4.4410071e+00, -3.6220193e+00, -7.7792563e+00, -3.7984440e+00,\n",
       "       -5.9836423e-01, -6.0381422e+00, -8.0033770e+00,  5.9778702e-01,\n",
       "       -2.9042332e+00, -1.0831542e+01, -1.0103655e+01, -2.1359751e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset[:,:-1], y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lungs_train = []\n",
    "# for i in range(X_train[:,0].shape[0]):\n",
    "#     lungs_train.append(X_train[:,0][0])\n",
    "# lungs_train = tf.convert_to_tensor(lungs_train)\n",
    "# lungs_train = tf.reshape(lungs_train, (X_train[:,0].shape[0], 7, 512, 512, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lungs_train = []\n",
    "for i in range(X_train[:,0].shape[0]):\n",
    "    lungs_train.append(X_train[:,0][0]) #.astype(np.float32))\n",
    "\n",
    "lungs_train = np.array(lungs_train) #.astype(np.float32)   \n",
    "lungs_train = lungs_train.reshape(X_train[:,0].shape[0], 7, 512, 512, 1)\n",
    "\n",
    "lungs_test = []\n",
    "for i in range(X_test[:,0].shape[0]):\n",
    "    lungs_test.append(X_test[:,0][0]) #.astype(np.float32))\n",
    "\n",
    "lungs_test = np.array(lungs_test) #.astype(np.float32)    \n",
    "lungs_test = lungs_test.reshape(X_test[:,0].shape[0], 7, 512, 512, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features_train = []\n",
    "#for i in range(X_train[:,1:].shape[0]):\n",
    "#    features_train.append(X_train[i,1:]) #.astype(np.float32))\n",
    "#    \n",
    "#features_test = []\n",
    "#for i in range(X_test[:,1:].shape[0]):\n",
    "#    features_test.append(X_test[i,1:]) #.astype(np.float32))\n",
    "    \n",
    "features_train = []\n",
    "for i in range(X_train[:,1:].shape[0]):\n",
    "    features_train.append(X_train[:,1:][0])\n",
    "features_train = tf.convert_to_tensor(features_train)\n",
    "features_train = tf.reshape(features_train, (features_train.shape[0],8))\n",
    "\n",
    "features_test = []\n",
    "for i in range(X_test[:,1:].shape[0]):\n",
    "    features_test.append(X_test[:,1:][0])\n",
    "features_test = tf.convert_to_tensor(features_test)\n",
    "features_test = tf.reshape(features_test, (features_test.shape[0],8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = []\n",
    "for i in range(X_train[:,1:].shape[0]):\n",
    "    features_train.append(X_train[:,1:][0])\n",
    "features_train = np.array(np.array(features_train)).astype(np.float32)\n",
    "features_train = features_train.reshape(features_train.shape[0], 8)\n",
    "\n",
    "features_test = []\n",
    "for i in range(X_test[:,1:].shape[0]):\n",
    "    features_test.append(X_test[:,1:][0])\n",
    "features_test = np.array(np.array(features_test)).astype(np.float32)\n",
    "features_test = features_test.reshape(features_test.shape[0], 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_conv = Input(shape = (7,512,512,1), name='lungs')\n",
    "\n",
    "model_conv = Conv3D(16, kernel_size=(1,2,2), strides=(1,1,1), padding='valid', activation='relu')(inp_conv)\n",
    "model_conv = Conv3D(8, kernel_size=(1,3,3), strides=(1,1,1), padding='valid', activation='relu')(model_conv)\n",
    "model_conv = BatchNormalization()(model_conv)\n",
    "model_conv = MaxPool3D(pool_size=(2,3,3))(model_conv)\n",
    "model_conv = Dropout(0.25)(model_conv)\n",
    "model_conv = Conv3D(16, kernel_size=(2,1,2), strides=(1,1,1), padding='valid', activation='relu')(model_conv)\n",
    "model_conv = Conv3D(16, kernel_size=(2,2,1), strides=(1,1,1), padding='valid', activation='relu')(model_conv)\n",
    "model_conv = BatchNormalization()(model_conv)\n",
    "model_conv = AveragePooling3D(pool_size=(1,3,3))(model_conv)\n",
    "model_conv = Dense(8, activation='relu')(model_conv)\n",
    "model_conv = MaxPool3D(pool_size=(1,3,3))(model_conv)\n",
    "model_conv = Dropout(0.25)(model_conv)\n",
    "model_conv = Flatten()(model_conv)\n",
    "model_conv = Dense(8, activation='relu')(model_conv)\n",
    "outp_conv = Dense(8, activation='sigmoid')(model_conv)\n",
    "\n",
    "inp_feat = Input(shape = (8,), name='features')\n",
    "model_feat = Dense(16, activation='relu')(inp_feat)\n",
    "model_feat = Dense(32, activation='relu')(model_feat)\n",
    "outp_feat = Dense(8, activation='sigmoid')(model_feat)\n",
    "\n",
    "model_conc = concatenate([outp_conv, outp_feat])\n",
    "model_conc = Dense(16, activation='relu')(model_conc)\n",
    "model_conc = Dense(32, activation='relu')(model_conc)\n",
    "model_conc = Dense(8, activation='relu')(model_conc)\n",
    "output = Dense(1, activation='linear')(model_conc)\n",
    "\n",
    "model = Model(inputs=[inp_conv, inp_feat], outputs=output, name=\"cnn_nn_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"cnn_nn_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "lungs (InputLayer)              [(None, 7, 512, 512, 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_44 (Conv3D)              (None, 7, 511, 511,  80          lungs[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_45 (Conv3D)              (None, 7, 509, 509,  1160        conv3d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 7, 509, 509,  32          conv3d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling3d_21 (MaxPooling3D) (None, 3, 169, 169,  0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)            (None, 3, 169, 169,  0           max_pooling3d_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_46 (Conv3D)              (None, 2, 169, 168,  528         dropout_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_47 (Conv3D)              (None, 1, 168, 168,  1040        conv3d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 1, 168, 168,  64          conv3d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling3d_11 (AveragePo (None, 1, 56, 56, 16 0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_80 (Dense)                (None, 1, 56, 56, 8) 136         average_pooling3d_11[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling3d_22 (MaxPooling3D) (None, 1, 18, 18, 8) 0           dense_80[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_21 (Dropout)            (None, 1, 18, 18, 8) 0           max_pooling3d_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "features (InputLayer)           [(None, 8)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)             (None, 2592)         0           dropout_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_83 (Dense)                (None, 16)           144         features[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_81 (Dense)                (None, 8)            20744       flatten_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_84 (Dense)                (None, 32)           544         dense_83[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_82 (Dense)                (None, 8)            72          dense_81[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_85 (Dense)                (None, 8)            264         dense_84[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 16)           0           dense_82[0][0]                   \n",
      "                                                                 dense_85[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_86 (Dense)                (None, 16)           272         concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_87 (Dense)                (None, 32)           544         dense_86[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_88 (Dense)                (None, 8)            264         dense_87[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_89 (Dense)                (None, 1)            9           dense_88[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 25,897\n",
      "Trainable params: 25,849\n",
      "Non-trainable params: 48\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=custom_loss_function(), metrics=[custom_metric_function()], optimizer='adam')\n",
    "#model.compile(loss='mse', metrics=['mse'],optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      " 2/18 [==>...........................] - ETA: 5s - loss: 4.3412 - metric_function: 3.5176WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.2288s vs `on_train_batch_end` time: 0.4036s). Check your callbacks.\n",
      "18/18 [==============================] - 14s 788ms/step - loss: 5.2682 - metric_function: 4.1313 - val_loss: 6.0476 - val_metric_function: 4.5268\n",
      "Epoch 2/200\n",
      "18/18 [==============================] - 12s 661ms/step - loss: 4.9775 - metric_function: 3.9482 - val_loss: 5.7293 - val_metric_function: 4.3228\n",
      "Epoch 3/200\n",
      "18/18 [==============================] - 12s 664ms/step - loss: 4.7044 - metric_function: 3.7755 - val_loss: 5.4118 - val_metric_function: 4.0984\n",
      "Epoch 4/200\n",
      "18/18 [==============================] - 12s 661ms/step - loss: 4.4438 - metric_function: 3.6084 - val_loss: 5.1687 - val_metric_function: 3.9282\n",
      "Epoch 5/200\n",
      "18/18 [==============================] - 12s 660ms/step - loss: 4.2757 - metric_function: 3.5059 - val_loss: 4.9912 - val_metric_function: 3.8285\n",
      "Epoch 6/200\n",
      "18/18 [==============================] - 12s 662ms/step - loss: 4.1938 - metric_function: 3.4842 - val_loss: 4.9316 - val_metric_function: 3.7945\n",
      "Epoch 7/200\n",
      "18/18 [==============================] - 12s 661ms/step - loss: 4.1866 - metric_function: 3.4952 - val_loss: 4.9138 - val_metric_function: 3.7612\n",
      "Epoch 8/200\n",
      "18/18 [==============================] - 12s 663ms/step - loss: 4.1956 - metric_function: 3.5220 - val_loss: 4.9138 - val_metric_function: 3.7592\n",
      "Epoch 9/200\n",
      "18/18 [==============================] - 12s 665ms/step - loss: 4.1856 - metric_function: 3.5053 - val_loss: 4.9197 - val_metric_function: 3.7768\n",
      "Epoch 10/200\n",
      "18/18 [==============================] - 12s 663ms/step - loss: 4.1882 - metric_function: 3.4932 - val_loss: 4.9249 - val_metric_function: 3.7872\n",
      "Epoch 11/200\n",
      "18/18 [==============================] - 12s 664ms/step - loss: 4.1949 - metric_function: 3.4984 - val_loss: 4.9275 - val_metric_function: 3.7905\n",
      "Epoch 12/200\n",
      "18/18 [==============================] - 12s 664ms/step - loss: 4.1894 - metric_function: 3.4970 - val_loss: 4.9163 - val_metric_function: 3.7701\n",
      "Epoch 13/200\n",
      "18/18 [==============================] - 12s 664ms/step - loss: 4.1946 - metric_function: 3.5134 - val_loss: 4.9149 - val_metric_function: 3.7673\n",
      "Epoch 14/200\n",
      "18/18 [==============================] - 12s 665ms/step - loss: 4.1961 - metric_function: 3.5033 - val_loss: 4.9221 - val_metric_function: 3.7817\n",
      "Epoch 15/200\n",
      "18/18 [==============================] - 12s 664ms/step - loss: 4.1916 - metric_function: 3.4989 - val_loss: 4.9189 - val_metric_function: 3.7753\n",
      "Epoch 16/200\n",
      "18/18 [==============================] - 12s 666ms/step - loss: 4.1956 - metric_function: 3.5085 - val_loss: 4.9171 - val_metric_function: 3.7716\n",
      "Epoch 17/200\n",
      "18/18 [==============================] - 12s 666ms/step - loss: 4.1892 - metric_function: 3.5065 - val_loss: 4.9139 - val_metric_function: 3.7653\n",
      "Epoch 18/200\n",
      "18/18 [==============================] - 12s 664ms/step - loss: 4.1948 - metric_function: 3.5000 - val_loss: 4.9218 - val_metric_function: 3.7811\n",
      "Epoch 19/200\n",
      "18/18 [==============================] - 12s 665ms/step - loss: 4.1894 - metric_function: 3.4963 - val_loss: 4.9196 - val_metric_function: 3.7767\n",
      "Epoch 20/200\n",
      "18/18 [==============================] - 12s 665ms/step - loss: 4.1920 - metric_function: 3.5136 - val_loss: 4.9138 - val_metric_function: 3.7612\n",
      "Epoch 21/200\n",
      "18/18 [==============================] - 12s 666ms/step - loss: 4.1899 - metric_function: 3.5139 - val_loss: 4.9138 - val_metric_function: 3.7600\n",
      "Epoch 22/200\n",
      "18/18 [==============================] - 12s 666ms/step - loss: 4.1921 - metric_function: 3.5194 - val_loss: 4.9138 - val_metric_function: 3.7603\n",
      "Epoch 23/200\n",
      "18/18 [==============================] - 12s 664ms/step - loss: 4.1893 - metric_function: 3.5129 - val_loss: 4.9139 - val_metric_function: 3.7653\n",
      "Epoch 24/200\n",
      "18/18 [==============================] - 12s 666ms/step - loss: 4.1930 - metric_function: 3.5042 - val_loss: 4.9218 - val_metric_function: 3.7810\n",
      "Epoch 25/200\n",
      "18/18 [==============================] - 12s 665ms/step - loss: 4.2013 - metric_function: 3.5162 - val_loss: 4.9138 - val_metric_function: 3.7644\n",
      "Epoch 26/200\n",
      "18/18 [==============================] - 12s 664ms/step - loss: 4.1958 - metric_function: 3.5199 - val_loss: 4.9138 - val_metric_function: 3.7637\n",
      "Epoch 27/200\n",
      "18/18 [==============================] - 12s 663ms/step - loss: 4.2054 - metric_function: 3.5072 - val_loss: 4.9232 - val_metric_function: 3.7839\n",
      "Epoch 28/200\n",
      "18/18 [==============================] - 12s 666ms/step - loss: 4.1914 - metric_function: 3.4956 - val_loss: 4.9247 - val_metric_function: 3.7869\n",
      "Epoch 29/200\n",
      "18/18 [==============================] - 12s 667ms/step - loss: 4.1894 - metric_function: 3.4978 - val_loss: 4.9185 - val_metric_function: 3.7745\n",
      "Epoch 30/200\n",
      "18/18 [==============================] - 12s 665ms/step - loss: 4.1879 - metric_function: 3.5035 - val_loss: 4.9149 - val_metric_function: 3.7673\n",
      "Epoch 31/200\n",
      "18/18 [==============================] - 12s 665ms/step - loss: 4.1902 - metric_function: 3.5055 - val_loss: 4.9164 - val_metric_function: 3.7704\n",
      "Epoch 32/200\n",
      "18/18 [==============================] - 12s 666ms/step - loss: 4.2060 - metric_function: 3.5029 - val_loss: 4.9232 - val_metric_function: 3.7838\n",
      "Epoch 33/200\n",
      "18/18 [==============================] - 12s 665ms/step - loss: 4.1883 - metric_function: 3.5068 - val_loss: 4.9138 - val_metric_function: 3.7617\n",
      "Epoch 34/200\n",
      "18/18 [==============================] - 12s 666ms/step - loss: 4.1974 - metric_function: 3.5262 - val_loss: 4.9138 - val_metric_function: 3.7602\n",
      "Epoch 35/200\n",
      "18/18 [==============================] - 12s 666ms/step - loss: 4.1925 - metric_function: 3.5147 - val_loss: 4.9139 - val_metric_function: 3.7619\n",
      "Epoch 36/200\n",
      "18/18 [==============================] - 12s 666ms/step - loss: 4.1974 - metric_function: 3.5043 - val_loss: 4.9303 - val_metric_function: 3.7932\n",
      "Epoch 37/200\n",
      "18/18 [==============================] - 12s 666ms/step - loss: 4.1943 - metric_function: 3.4989 - val_loss: 4.9208 - val_metric_function: 3.7790\n",
      "Epoch 38/200\n",
      "18/18 [==============================] - 12s 664ms/step - loss: 4.1992 - metric_function: 3.5005 - val_loss: 4.9317 - val_metric_function: 3.7947\n",
      "Epoch 39/200\n",
      "18/18 [==============================] - 12s 666ms/step - loss: 4.2229 - metric_function: 3.5380 - val_loss: 4.9138 - val_metric_function: 3.7594\n",
      "Epoch 40/200\n",
      "18/18 [==============================] - 12s 663ms/step - loss: 4.1952 - metric_function: 3.5108 - val_loss: 4.9178 - val_metric_function: 3.7731\n",
      "Epoch 41/200\n",
      "18/18 [==============================] - 12s 667ms/step - loss: 4.1937 - metric_function: 3.5073 - val_loss: 4.9216 - val_metric_function: 3.7808\n",
      "Epoch 42/200\n",
      "18/18 [==============================] - 12s 665ms/step - loss: 4.1962 - metric_function: 3.5055 - val_loss: 4.9171 - val_metric_function: 3.7717\n",
      "Epoch 43/200\n",
      "18/18 [==============================] - 12s 666ms/step - loss: 4.1981 - metric_function: 3.5113 - val_loss: 4.9185 - val_metric_function: 3.7745\n",
      "Epoch 44/200\n",
      "18/18 [==============================] - 12s 663ms/step - loss: 4.1946 - metric_function: 3.4960 - val_loss: 4.9273 - val_metric_function: 3.7902\n",
      "Epoch 45/200\n",
      "18/18 [==============================] - 12s 665ms/step - loss: 4.1906 - metric_function: 3.4934 - val_loss: 4.9239 - val_metric_function: 3.7853\n",
      "Epoch 46/200\n",
      "18/18 [==============================] - 12s 667ms/step - loss: 4.1985 - metric_function: 3.4976 - val_loss: 4.9344 - val_metric_function: 3.7973\n",
      "Epoch 47/200\n",
      "18/18 [==============================] - 12s 669ms/step - loss: 4.2050 - metric_function: 3.5140 - val_loss: 4.9198 - val_metric_function: 3.7770\n",
      "Epoch 48/200\n",
      "18/18 [==============================] - 12s 672ms/step - loss: 4.1970 - metric_function: 3.5029 - val_loss: 4.9191 - val_metric_function: 3.7756\n",
      "Epoch 49/200\n",
      "18/18 [==============================] - 12s 673ms/step - loss: 4.1932 - metric_function: 3.4986 - val_loss: 4.9257 - val_metric_function: 3.7886\n",
      "Epoch 50/200\n",
      "18/18 [==============================] - 12s 665ms/step - loss: 4.1919 - metric_function: 3.4966 - val_loss: 4.9210 - val_metric_function: 3.7795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/200\n",
      "18/18 [==============================] - 12s 665ms/step - loss: 4.1902 - metric_function: 3.5021 - val_loss: 4.9163 - val_metric_function: 3.7700\n",
      "Epoch 52/200\n",
      "18/18 [==============================] - 12s 664ms/step - loss: 4.1896 - metric_function: 3.5003 - val_loss: 4.9232 - val_metric_function: 3.7839\n",
      "Epoch 53/200\n",
      "18/18 [==============================] - 12s 666ms/step - loss: 4.1984 - metric_function: 3.5014 - val_loss: 4.9249 - val_metric_function: 3.7873\n",
      "Epoch 54/200\n",
      "18/18 [==============================] - 12s 670ms/step - loss: 4.1914 - metric_function: 3.5077 - val_loss: 4.9138 - val_metric_function: 3.7631\n",
      "Epoch 55/200\n",
      "18/18 [==============================] - 12s 667ms/step - loss: 4.1928 - metric_function: 3.5092 - val_loss: 4.9185 - val_metric_function: 3.7745\n",
      "Epoch 56/200\n",
      "18/18 [==============================] - 12s 673ms/step - loss: 4.1879 - metric_function: 3.5008 - val_loss: 4.9172 - val_metric_function: 3.7718\n",
      "Epoch 57/200\n",
      "18/18 [==============================] - 12s 668ms/step - loss: 4.1894 - metric_function: 3.5020 - val_loss: 4.9171 - val_metric_function: 3.7718\n",
      "Epoch 58/200\n",
      "18/18 [==============================] - 12s 669ms/step - loss: 4.1888 - metric_function: 3.5071 - val_loss: 4.9148 - val_metric_function: 3.7670\n",
      "Epoch 59/200\n",
      "18/18 [==============================] - 12s 664ms/step - loss: 4.1934 - metric_function: 3.5102 - val_loss: 4.9193 - val_metric_function: 3.7761\n",
      "Epoch 60/200\n",
      "18/18 [==============================] - 12s 668ms/step - loss: 4.2074 - metric_function: 3.5295 - val_loss: 4.9138 - val_metric_function: 3.7618\n",
      "Epoch 61/200\n",
      "18/18 [==============================] - 12s 664ms/step - loss: 4.1886 - metric_function: 3.5062 - val_loss: 4.9175 - val_metric_function: 3.7725\n",
      "Epoch 62/200\n",
      "18/18 [==============================] - 12s 666ms/step - loss: 4.1908 - metric_function: 3.4988 - val_loss: 4.9234 - val_metric_function: 3.7844\n",
      "Epoch 63/200\n",
      "18/18 [==============================] - 12s 665ms/step - loss: 4.1893 - metric_function: 3.4945 - val_loss: 4.9207 - val_metric_function: 3.7790\n",
      "Epoch 64/200\n",
      "18/18 [==============================] - 12s 665ms/step - loss: 4.1888 - metric_function: 3.4998 - val_loss: 4.9150 - val_metric_function: 3.7674\n",
      "Epoch 65/200\n",
      "18/18 [==============================] - 12s 669ms/step - loss: 4.1928 - metric_function: 3.5115 - val_loss: 4.9175 - val_metric_function: 3.7725\n",
      "Epoch 66/200\n",
      "18/18 [==============================] - 12s 684ms/step - loss: 4.1988 - metric_function: 3.5188 - val_loss: 4.9138 - val_metric_function: 3.7645\n",
      "Epoch 67/200\n",
      "18/18 [==============================] - 12s 684ms/step - loss: 4.1889 - metric_function: 3.5030 - val_loss: 4.9205 - val_metric_function: 3.7785\n",
      "Epoch 68/200\n",
      "18/18 [==============================] - 12s 666ms/step - loss: 4.1936 - metric_function: 3.5098 - val_loss: 4.9157 - val_metric_function: 3.7689\n",
      "Epoch 69/200\n",
      "18/18 [==============================] - 12s 664ms/step - loss: 4.1882 - metric_function: 3.5026 - val_loss: 4.9187 - val_metric_function: 3.7748\n",
      "Epoch 70/200\n",
      "18/18 [==============================] - 12s 669ms/step - loss: 4.1881 - metric_function: 3.5012 - val_loss: 4.9177 - val_metric_function: 3.7728\n",
      "Epoch 71/200\n",
      "18/18 [==============================] - 12s 665ms/step - loss: 4.1903 - metric_function: 3.5080 - val_loss: 4.9177 - val_metric_function: 3.7728\n",
      "Epoch 72/200\n",
      "18/18 [==============================] - 12s 668ms/step - loss: 4.1896 - metric_function: 3.4971 - val_loss: 4.9234 - val_metric_function: 3.7844\n",
      "Epoch 73/200\n",
      "18/18 [==============================] - 12s 667ms/step - loss: 4.1864 - metric_function: 3.4998 - val_loss: 4.9154 - val_metric_function: 3.7682\n",
      "Epoch 74/200\n",
      "18/18 [==============================] - 12s 665ms/step - loss: 4.1908 - metric_function: 3.5111 - val_loss: 4.9138 - val_metric_function: 3.7621\n",
      "Epoch 75/200\n",
      "18/18 [==============================] - 12s 666ms/step - loss: 4.1900 - metric_function: 3.5109 - val_loss: 4.9202 - val_metric_function: 3.7779\n",
      "Epoch 76/200\n",
      "18/18 [==============================] - 12s 665ms/step - loss: 4.2287 - metric_function: 3.5159 - val_loss: 4.9403 - val_metric_function: 3.8032\n",
      "Epoch 77/200\n",
      "18/18 [==============================] - 12s 666ms/step - loss: 4.1921 - metric_function: 3.5040 - val_loss: 4.9138 - val_metric_function: 3.7632\n",
      "Epoch 78/200\n",
      "18/18 [==============================] - 12s 665ms/step - loss: 4.1896 - metric_function: 3.5073 - val_loss: 4.9138 - val_metric_function: 3.7650\n",
      "Epoch 79/200\n",
      "18/18 [==============================] - 12s 667ms/step - loss: 4.1927 - metric_function: 3.5175 - val_loss: 4.9138 - val_metric_function: 3.7616\n",
      "Epoch 80/200\n",
      "18/18 [==============================] - 12s 666ms/step - loss: 4.1916 - metric_function: 3.5117 - val_loss: 4.9190 - val_metric_function: 3.7755\n",
      "Epoch 81/200\n",
      "18/18 [==============================] - 12s 667ms/step - loss: 4.1932 - metric_function: 3.4980 - val_loss: 4.9232 - val_metric_function: 3.7840\n",
      "Epoch 82/200\n",
      "18/18 [==============================] - 12s 668ms/step - loss: 4.1895 - metric_function: 3.4946 - val_loss: 4.9209 - val_metric_function: 3.7793\n",
      "Epoch 83/200\n",
      "18/18 [==============================] - 12s 665ms/step - loss: 4.2030 - metric_function: 3.5286 - val_loss: 4.9139 - val_metric_function: 3.7555\n",
      "Epoch 84/200\n",
      "18/18 [==============================] - 12s 668ms/step - loss: 4.1911 - metric_function: 3.5191 - val_loss: 4.9138 - val_metric_function: 3.7619\n",
      "Epoch 85/200\n",
      "18/18 [==============================] - 12s 666ms/step - loss: 4.1888 - metric_function: 3.5056 - val_loss: 4.9201 - val_metric_function: 3.7777\n",
      "Epoch 86/200\n",
      "18/18 [==============================] - 12s 667ms/step - loss: 4.1903 - metric_function: 3.5039 - val_loss: 4.9182 - val_metric_function: 3.7739\n",
      "Epoch 87/200\n",
      "18/18 [==============================] - 12s 664ms/step - loss: 4.1925 - metric_function: 3.4977 - val_loss: 4.9228 - val_metric_function: 3.7830\n",
      "Epoch 88/200\n",
      "18/18 [==============================] - 12s 665ms/step - loss: 4.1917 - metric_function: 3.5064 - val_loss: 4.9168 - val_metric_function: 3.7711\n",
      "Epoch 89/200\n",
      "18/18 [==============================] - 12s 665ms/step - loss: 4.2002 - metric_function: 3.5039 - val_loss: 4.9306 - val_metric_function: 3.7936\n",
      "Epoch 90/200\n",
      "18/18 [==============================] - 12s 666ms/step - loss: 4.2043 - metric_function: 3.5151 - val_loss: 4.9151 - val_metric_function: 3.7677\n",
      "Epoch 91/200\n",
      "18/18 [==============================] - 12s 664ms/step - loss: 4.1955 - metric_function: 3.5202 - val_loss: 4.9152 - val_metric_function: 3.7678\n",
      "Epoch 92/200\n",
      "18/18 [==============================] - 12s 666ms/step - loss: 4.1888 - metric_function: 3.5058 - val_loss: 4.9208 - val_metric_function: 3.7790\n",
      "Epoch 93/200\n",
      "18/18 [==============================] - 12s 666ms/step - loss: 4.1899 - metric_function: 3.4960 - val_loss: 4.9261 - val_metric_function: 3.7891\n",
      "Epoch 94/200\n",
      "18/18 [==============================] - 12s 673ms/step - loss: 4.1963 - metric_function: 3.4950 - val_loss: 4.9230 - val_metric_function: 3.7836\n",
      "Epoch 95/200\n",
      "18/18 [==============================] - 12s 675ms/step - loss: 4.1933 - metric_function: 3.5005 - val_loss: 4.9193 - val_metric_function: 3.7760\n",
      "Epoch 96/200\n",
      "18/18 [==============================] - 12s 686ms/step - loss: 4.1877 - metric_function: 3.5109 - val_loss: 4.9138 - val_metric_function: 3.7608\n",
      "Epoch 97/200\n",
      "18/18 [==============================] - 12s 672ms/step - loss: 4.1948 - metric_function: 3.5208 - val_loss: 4.9145 - val_metric_function: 3.7664\n",
      "Epoch 98/200\n",
      "18/18 [==============================] - 12s 672ms/step - loss: 4.1889 - metric_function: 3.5067 - val_loss: 4.9185 - val_metric_function: 3.7744\n",
      "Epoch 99/200\n",
      "18/18 [==============================] - 12s 665ms/step - loss: 4.1922 - metric_function: 3.5122 - val_loss: 4.9138 - val_metric_function: 3.7619\n",
      "Epoch 100/200\n",
      "18/18 [==============================] - 12s 666ms/step - loss: 4.1903 - metric_function: 3.5123 - val_loss: 4.9155 - val_metric_function: 3.7686\n",
      "Epoch 101/200\n",
      "18/18 [==============================] - 12s 670ms/step - loss: 4.1901 - metric_function: 3.5064 - val_loss: 4.9177 - val_metric_function: 3.7728\n",
      "Epoch 102/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 12s 666ms/step - loss: 4.1905 - metric_function: 3.5026 - val_loss: 4.9178 - val_metric_function: 3.7731\n",
      "Epoch 103/200\n",
      "18/18 [==============================] - 12s 665ms/step - loss: 4.1920 - metric_function: 3.5117 - val_loss: 4.9138 - val_metric_function: 3.7624\n",
      "Epoch 104/200\n",
      "18/18 [==============================] - 12s 663ms/step - loss: 4.1888 - metric_function: 3.5037 - val_loss: 4.9215 - val_metric_function: 3.7805\n",
      "Epoch 105/200\n",
      "18/18 [==============================] - 12s 664ms/step - loss: 4.2032 - metric_function: 3.5030 - val_loss: 4.9258 - val_metric_function: 3.7888\n",
      "Epoch 106/200\n",
      "18/18 [==============================] - 12s 663ms/step - loss: 4.1962 - metric_function: 3.5020 - val_loss: 4.9225 - val_metric_function: 3.7824\n",
      "Epoch 107/200\n",
      "18/18 [==============================] - 12s 664ms/step - loss: 4.1868 - metric_function: 3.5000 - val_loss: 4.9155 - val_metric_function: 3.7684\n",
      "Epoch 108/200\n",
      "18/18 [==============================] - 12s 663ms/step - loss: 4.1940 - metric_function: 3.5183 - val_loss: 4.9138 - val_metric_function: 3.7613\n",
      "Epoch 109/200\n",
      "18/18 [==============================] - 12s 666ms/step - loss: 4.1908 - metric_function: 3.5130 - val_loss: 4.9138 - val_metric_function: 3.7618\n",
      "Epoch 110/200\n",
      "18/18 [==============================] - 12s 666ms/step - loss: 4.1893 - metric_function: 3.4982 - val_loss: 4.9223 - val_metric_function: 3.7820\n",
      "Epoch 111/200\n",
      "18/18 [==============================] - 12s 666ms/step - loss: 4.1967 - metric_function: 3.4980 - val_loss: 4.9220 - val_metric_function: 3.7814\n",
      "Epoch 112/200\n",
      "18/18 [==============================] - 12s 665ms/step - loss: 4.1892 - metric_function: 3.4989 - val_loss: 4.9196 - val_metric_function: 3.7767\n",
      "Epoch 113/200\n",
      "18/18 [==============================] - 12s 665ms/step - loss: 4.2018 - metric_function: 3.5256 - val_loss: 4.9138 - val_metric_function: 3.7568\n",
      "Epoch 114/200\n",
      "18/18 [==============================] - 12s 674ms/step - loss: 4.1912 - metric_function: 3.5152 - val_loss: 4.9154 - val_metric_function: 3.7682\n",
      "Epoch 115/200\n",
      "18/18 [==============================] - 12s 666ms/step - loss: 4.1876 - metric_function: 3.5036 - val_loss: 4.9187 - val_metric_function: 3.7748\n",
      "Epoch 116/200\n",
      "18/18 [==============================] - 12s 684ms/step - loss: 4.1921 - metric_function: 3.4984 - val_loss: 4.9187 - val_metric_function: 3.7748\n",
      "Epoch 117/200\n",
      "18/18 [==============================] - 12s 666ms/step - loss: 4.1938 - metric_function: 3.5009 - val_loss: 4.9188 - val_metric_function: 3.7752\n",
      "Epoch 118/200\n",
      "18/18 [==============================] - 12s 664ms/step - loss: 4.1896 - metric_function: 3.5021 - val_loss: 4.9163 - val_metric_function: 3.7701\n",
      "Epoch 119/200\n",
      "18/18 [==============================] - 12s 665ms/step - loss: 4.1930 - metric_function: 3.5047 - val_loss: 4.9178 - val_metric_function: 3.7732\n",
      "Epoch 120/200\n",
      "18/18 [==============================] - 12s 664ms/step - loss: 4.1922 - metric_function: 3.5109 - val_loss: 4.9151 - val_metric_function: 3.7677\n",
      "Epoch 121/200\n",
      "18/18 [==============================] - 12s 664ms/step - loss: 4.1954 - metric_function: 3.5143 - val_loss: 4.9138 - val_metric_function: 3.7610\n",
      "Epoch 122/200\n",
      "18/18 [==============================] - 12s 664ms/step - loss: 4.1903 - metric_function: 3.5031 - val_loss: 4.9196 - val_metric_function: 3.7768\n",
      "Epoch 123/200\n",
      "18/18 [==============================] - 12s 667ms/step - loss: 4.2011 - metric_function: 3.5131 - val_loss: 4.9138 - val_metric_function: 3.7629\n",
      "Epoch 124/200\n",
      "18/18 [==============================] - 12s 665ms/step - loss: 4.1934 - metric_function: 3.5059 - val_loss: 4.9215 - val_metric_function: 3.7806\n",
      "Epoch 125/200\n",
      "18/18 [==============================] - 12s 666ms/step - loss: 4.1877 - metric_function: 3.4992 - val_loss: 4.9162 - val_metric_function: 3.7698\n",
      "Epoch 126/200\n",
      "18/18 [==============================] - 12s 670ms/step - loss: 4.1937 - metric_function: 3.5025 - val_loss: 4.9198 - val_metric_function: 3.7771\n",
      "Epoch 127/200\n",
      "18/18 [==============================] - 12s 668ms/step - loss: 4.1917 - metric_function: 3.4959 - val_loss: 4.9243 - val_metric_function: 3.7860\n",
      "Epoch 128/200\n",
      "18/18 [==============================] - 12s 682ms/step - loss: 4.1948 - metric_function: 3.5013 - val_loss: 4.9232 - val_metric_function: 3.7838\n",
      "Epoch 129/200\n",
      "18/18 [==============================] - 12s 673ms/step - loss: 4.1924 - metric_function: 3.4944 - val_loss: 4.9233 - val_metric_function: 3.7841\n",
      "Epoch 130/200\n",
      "18/18 [==============================] - 12s 676ms/step - loss: 4.1988 - metric_function: 3.5139 - val_loss: 4.9138 - val_metric_function: 3.7617\n",
      "Epoch 131/200\n",
      "18/18 [==============================] - 12s 681ms/step - loss: 4.2238 - metric_function: 3.5231 - val_loss: 4.9401 - val_metric_function: 3.8030\n",
      "Epoch 132/200\n",
      "18/18 [==============================] - 12s 675ms/step - loss: 4.1947 - metric_function: 3.4971 - val_loss: 4.9197 - val_metric_function: 3.7768\n",
      "Epoch 133/200\n",
      "18/18 [==============================] - 12s 685ms/step - loss: 4.1928 - metric_function: 3.5062 - val_loss: 4.9188 - val_metric_function: 3.7751\n",
      "Epoch 134/200\n",
      "18/18 [==============================] - 12s 669ms/step - loss: 4.1891 - metric_function: 3.5003 - val_loss: 4.9166 - val_metric_function: 3.7706\n",
      "Epoch 135/200\n",
      "18/18 [==============================] - 12s 666ms/step - loss: 4.1948 - metric_function: 3.5131 - val_loss: 4.9138 - val_metric_function: 3.7635\n",
      "Epoch 136/200\n",
      "18/18 [==============================] - 12s 665ms/step - loss: 4.1899 - metric_function: 3.5043 - val_loss: 4.9208 - val_metric_function: 3.7790\n",
      "Epoch 137/200\n",
      "18/18 [==============================] - 12s 669ms/step - loss: 4.1980 - metric_function: 3.5112 - val_loss: 4.9164 - val_metric_function: 3.7703\n",
      "Epoch 138/200\n",
      "18/18 [==============================] - 12s 669ms/step - loss: 4.1899 - metric_function: 3.5054 - val_loss: 4.9173 - val_metric_function: 3.7721\n",
      "Epoch 139/200\n",
      "18/18 [==============================] - 12s 664ms/step - loss: 4.1999 - metric_function: 3.5050 - val_loss: 4.9202 - val_metric_function: 3.7778\n",
      "Epoch 140/200\n",
      "18/18 [==============================] - 12s 665ms/step - loss: 4.1916 - metric_function: 3.4947 - val_loss: 4.9269 - val_metric_function: 3.7898\n",
      "Epoch 141/200\n",
      "18/18 [==============================] - 12s 681ms/step - loss: 4.1912 - metric_function: 3.4941 - val_loss: 4.9252 - val_metric_function: 3.7879\n",
      "Epoch 142/200\n",
      "18/18 [==============================] - 12s 677ms/step - loss: 4.1900 - metric_function: 3.4945 - val_loss: 4.9231 - val_metric_function: 3.7837\n",
      "Epoch 143/200\n",
      "18/18 [==============================] - 12s 661ms/step - loss: 4.1897 - metric_function: 3.4958 - val_loss: 4.9214 - val_metric_function: 3.7802\n",
      "Epoch 144/200\n",
      "18/18 [==============================] - 12s 678ms/step - loss: 4.1885 - metric_function: 3.4997 - val_loss: 4.9181 - val_metric_function: 3.7737\n",
      "Epoch 145/200\n",
      "18/18 [==============================] - 12s 680ms/step - loss: 4.1902 - metric_function: 3.5087 - val_loss: 4.9145 - val_metric_function: 3.7664\n",
      "Epoch 146/200\n",
      "18/18 [==============================] - 12s 667ms/step - loss: 4.1971 - metric_function: 3.5007 - val_loss: 4.9222 - val_metric_function: 3.7819\n",
      "Epoch 147/200\n",
      "18/18 [==============================] - 12s 679ms/step - loss: 4.1885 - metric_function: 3.4979 - val_loss: 4.9188 - val_metric_function: 3.7750\n",
      "Epoch 148/200\n",
      "18/18 [==============================] - 12s 682ms/step - loss: 4.1876 - metric_function: 3.5006 - val_loss: 4.9185 - val_metric_function: 3.7746\n",
      "Epoch 149/200\n",
      "18/18 [==============================] - 13s 704ms/step - loss: 4.1876 - metric_function: 3.5010 - val_loss: 4.9180 - val_metric_function: 3.7736\n",
      "Epoch 150/200\n",
      "18/18 [==============================] - 13s 704ms/step - loss: 4.2036 - metric_function: 3.5045 - val_loss: 4.9246 - val_metric_function: 3.7868\n",
      "Epoch 151/200\n",
      "18/18 [==============================] - 13s 705ms/step - loss: 4.1892 - metric_function: 3.4932 - val_loss: 4.9214 - val_metric_function: 3.7804\n",
      "Epoch 152/200\n",
      "18/18 [==============================] - 12s 693ms/step - loss: 4.1867 - metric_function: 3.4969 - val_loss: 4.9143 - val_metric_function: 3.7661\n",
      "Epoch 153/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 12s 674ms/step - loss: 4.1918 - metric_function: 3.5154 - val_loss: 4.9138 - val_metric_function: 3.7609\n",
      "Epoch 154/200\n",
      "18/18 [==============================] - 12s 672ms/step - loss: 4.1968 - metric_function: 3.5196 - val_loss: 4.9152 - val_metric_function: 3.7679\n",
      "Epoch 155/200\n",
      "18/18 [==============================] - 12s 686ms/step - loss: 4.1912 - metric_function: 3.4995 - val_loss: 4.9207 - val_metric_function: 3.7789\n",
      "Epoch 156/200\n",
      "18/18 [==============================] - 12s 672ms/step - loss: 4.1867 - metric_function: 3.4980 - val_loss: 4.9165 - val_metric_function: 3.7704\n",
      "Epoch 157/200\n",
      "18/18 [==============================] - 12s 662ms/step - loss: 4.1902 - metric_function: 3.5113 - val_loss: 4.9138 - val_metric_function: 3.7615\n",
      "Epoch 158/200\n",
      "18/18 [==============================] - 12s 689ms/step - loss: 4.2072 - metric_function: 3.5083 - val_loss: 4.9221 - val_metric_function: 3.7817\n",
      "Epoch 159/200\n",
      "18/18 [==============================] - 12s 682ms/step - loss: 4.1900 - metric_function: 3.5058 - val_loss: 4.9139 - val_metric_function: 3.7653\n",
      "Epoch 160/200\n",
      "18/18 [==============================] - 12s 684ms/step - loss: 4.1930 - metric_function: 3.5129 - val_loss: 4.9147 - val_metric_function: 3.7669\n",
      "Epoch 161/200\n",
      "18/18 [==============================] - 12s 680ms/step - loss: 4.1884 - metric_function: 3.5061 - val_loss: 4.9173 - val_metric_function: 3.7721\n",
      "Epoch 162/200\n",
      "18/18 [==============================] - 13s 699ms/step - loss: 4.1906 - metric_function: 3.5027 - val_loss: 4.9171 - val_metric_function: 3.7716\n",
      "Epoch 163/200\n",
      "18/18 [==============================] - 13s 706ms/step - loss: 4.1889 - metric_function: 3.5017 - val_loss: 4.9173 - val_metric_function: 3.7720\n",
      "Epoch 164/200\n",
      "18/18 [==============================] - 12s 676ms/step - loss: 4.1914 - metric_function: 3.4999 - val_loss: 4.9206 - val_metric_function: 3.7786\n",
      "Epoch 165/200\n",
      "18/18 [==============================] - 12s 669ms/step - loss: 4.1908 - metric_function: 3.5049 - val_loss: 4.9189 - val_metric_function: 3.7752\n",
      "Epoch 166/200\n",
      "18/18 [==============================] - 12s 672ms/step - loss: 4.1890 - metric_function: 3.5003 - val_loss: 4.9197 - val_metric_function: 3.7770\n",
      "Epoch 167/200\n",
      "18/18 [==============================] - 12s 687ms/step - loss: 4.1906 - metric_function: 3.4995 - val_loss: 4.9204 - val_metric_function: 3.7783\n",
      "Epoch 168/200\n",
      "18/18 [==============================] - 13s 701ms/step - loss: 4.2038 - metric_function: 3.5215 - val_loss: 4.9138 - val_metric_function: 3.7625\n",
      "Epoch 169/200\n",
      "18/18 [==============================] - 12s 671ms/step - loss: 4.1990 - metric_function: 3.5095 - val_loss: 4.9176 - val_metric_function: 3.7728\n",
      "Epoch 170/200\n",
      "18/18 [==============================] - 12s 673ms/step - loss: 4.1878 - metric_function: 3.5011 - val_loss: 4.9163 - val_metric_function: 3.7701\n",
      "Epoch 171/200\n",
      "18/18 [==============================] - 12s 677ms/step - loss: 4.1921 - metric_function: 3.5035 - val_loss: 4.9172 - val_metric_function: 3.7720\n",
      "Epoch 172/200\n",
      "18/18 [==============================] - 12s 682ms/step - loss: 4.1901 - metric_function: 3.5043 - val_loss: 4.9160 - val_metric_function: 3.7694\n",
      "Epoch 173/200\n",
      "18/18 [==============================] - 12s 680ms/step - loss: 4.1893 - metric_function: 3.5039 - val_loss: 4.9176 - val_metric_function: 3.7726\n",
      "Epoch 174/200\n",
      "18/18 [==============================] - 12s 681ms/step - loss: 4.1887 - metric_function: 3.5041 - val_loss: 4.9153 - val_metric_function: 3.7681\n",
      "Epoch 175/200\n",
      "18/18 [==============================] - 12s 679ms/step - loss: 4.1998 - metric_function: 3.5091 - val_loss: 4.9188 - val_metric_function: 3.7752\n",
      "Epoch 176/200\n",
      "18/18 [==============================] - 12s 675ms/step - loss: 4.1876 - metric_function: 3.5050 - val_loss: 4.9138 - val_metric_function: 3.7633\n",
      "Epoch 177/200\n",
      "18/18 [==============================] - 12s 692ms/step - loss: 4.2024 - metric_function: 3.5100 - val_loss: 4.9182 - val_metric_function: 3.7738\n",
      "Epoch 178/200\n",
      "18/18 [==============================] - 12s 686ms/step - loss: 4.1884 - metric_function: 3.5031 - val_loss: 4.9149 - val_metric_function: 3.7674\n",
      "Epoch 179/200\n",
      "18/18 [==============================] - 12s 688ms/step - loss: 4.1958 - metric_function: 3.5059 - val_loss: 4.9166 - val_metric_function: 3.7707\n",
      "Epoch 180/200\n",
      "18/18 [==============================] - 12s 684ms/step - loss: 4.1933 - metric_function: 3.5002 - val_loss: 4.9251 - val_metric_function: 3.7877\n",
      "Epoch 181/200\n",
      "18/18 [==============================] - 12s 682ms/step - loss: 4.1889 - metric_function: 3.4915 - val_loss: 4.9230 - val_metric_function: 3.7835\n",
      "Epoch 182/200\n",
      "18/18 [==============================] - 12s 681ms/step - loss: 4.1929 - metric_function: 3.5104 - val_loss: 4.9138 - val_metric_function: 3.7629\n",
      "Epoch 183/200\n",
      "18/18 [==============================] - 12s 668ms/step - loss: 4.1975 - metric_function: 3.5264 - val_loss: 4.9138 - val_metric_function: 3.7585\n",
      "Epoch 184/200\n",
      "18/18 [==============================] - 12s 670ms/step - loss: 4.1926 - metric_function: 3.5096 - val_loss: 4.9206 - val_metric_function: 3.7786\n",
      "Epoch 185/200\n",
      "18/18 [==============================] - 12s 673ms/step - loss: 4.1886 - metric_function: 3.5001 - val_loss: 4.9189 - val_metric_function: 3.7753\n",
      "Epoch 186/200\n",
      "18/18 [==============================] - 12s 680ms/step - loss: 4.1884 - metric_function: 3.4980 - val_loss: 4.9204 - val_metric_function: 3.7782\n",
      "Epoch 187/200\n",
      "18/18 [==============================] - 12s 685ms/step - loss: 4.1890 - metric_function: 3.4985 - val_loss: 4.9184 - val_metric_function: 3.7743\n",
      "Epoch 188/200\n",
      "18/18 [==============================] - 12s 686ms/step - loss: 4.1889 - metric_function: 3.5043 - val_loss: 4.9139 - val_metric_function: 3.7615\n",
      "Epoch 189/200\n",
      "18/18 [==============================] - 12s 683ms/step - loss: 4.1889 - metric_function: 3.5108 - val_loss: 4.9138 - val_metric_function: 3.7647\n",
      "Epoch 190/200\n",
      "18/18 [==============================] - 12s 668ms/step - loss: 4.1887 - metric_function: 3.5008 - val_loss: 4.9228 - val_metric_function: 3.7831\n",
      "Epoch 191/200\n",
      "18/18 [==============================] - 12s 688ms/step - loss: 4.1907 - metric_function: 3.5001 - val_loss: 4.9186 - val_metric_function: 3.7747\n",
      "Epoch 192/200\n",
      "18/18 [==============================] - 13s 697ms/step - loss: 4.1964 - metric_function: 3.4994 - val_loss: 4.9241 - val_metric_function: 3.7856\n",
      "Epoch 193/200\n",
      "18/18 [==============================] - 12s 675ms/step - loss: 4.1874 - metric_function: 3.5001 - val_loss: 4.9160 - val_metric_function: 3.7695\n",
      "Epoch 194/200\n",
      "18/18 [==============================] - 12s 682ms/step - loss: 4.1920 - metric_function: 3.5089 - val_loss: 4.9183 - val_metric_function: 3.7740\n",
      "Epoch 195/200\n",
      "18/18 [==============================] - 12s 680ms/step - loss: 4.1957 - metric_function: 3.5155 - val_loss: 4.9138 - val_metric_function: 3.7605\n",
      "Epoch 196/200\n",
      "18/18 [==============================] - 12s 683ms/step - loss: 4.1894 - metric_function: 3.5158 - val_loss: 4.9138 - val_metric_function: 3.7642\n",
      "Epoch 197/200\n",
      "18/18 [==============================] - 12s 675ms/step - loss: 4.1956 - metric_function: 3.5082 - val_loss: 4.9196 - val_metric_function: 3.7767\n",
      "Epoch 198/200\n",
      "18/18 [==============================] - 12s 685ms/step - loss: 4.1889 - metric_function: 3.4977 - val_loss: 4.9214 - val_metric_function: 3.7803\n",
      "Epoch 199/200\n",
      "18/18 [==============================] - 12s 687ms/step - loss: 4.1899 - metric_function: 3.4971 - val_loss: 4.9185 - val_metric_function: 3.7745\n",
      "Epoch 200/200\n",
      "18/18 [==============================] - 12s 680ms/step - loss: 4.1904 - metric_function: 3.5002 - val_loss: 4.9195 - val_metric_function: 3.7765\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fad147832d0>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    {\"lungs\": lungs_train, \"features\": features_train},\n",
    "    y_train,\n",
    "    epochs=200, #experimenta mudar este número\n",
    "    batch_size=8,\n",
    "    validation_data = ({\"lungs\": lungs_test, \"features\": features_test}, y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.771681],\n",
       "       [-3.771681],\n",
       "       [-3.771681],\n",
       "       [-3.771681],\n",
       "       [-3.771681]], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(([lungs_test[0:5,:,:,:,:], features_test[0:5,:]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-18.381779 ,   9.052208 ,  -4.740104 ,  -3.6651213,  -3.3800275],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
